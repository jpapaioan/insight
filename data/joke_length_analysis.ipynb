{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import everything\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS, remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "from profanity_check import predict, predict_prob\n",
    "from itertools import compress\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in pre-trained word2vec model\n",
    "import gensim.downloader as api\n",
    "w2v_path = '~/gensim-data/word2vec-google-news-300'\n",
    "w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection of annoying punctuation specific to these comedy transcripts\n",
    "def init_clean(single_special):\n",
    "    single_special = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", single_special)\n",
    "    single_special = re.sub(\"-\", \" \", single_special)\n",
    "    single_special = re.sub(\"♪\", \"\", single_special)\n",
    "    return single_special\n",
    "\n",
    "# filter out sentences that are 2 or less words!\n",
    "def short_sents_filter(text):\n",
    "    for sentence in text:\n",
    "        words = sentence.split(\" \")\n",
    "        if len(words) < 3:\n",
    "            text.remove(sentence)\n",
    "    return text\n",
    "\n",
    "\n",
    "# lemmatizing\n",
    "stemmer = SnowballStemmer('english')\n",
    "#comedy_cliche = ['like', 'know', 'say', 'look', 'come', 'right', 'go', 'think']\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            #if token not in comedy_cliche:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def profane_filter(text):\n",
    "    cleaned_up = predict(text)\n",
    "    cleaned_up = 1 - cleaned_up\n",
    "    clean_list = list(compress(text, cleaned_up))\n",
    "    cleaned_up_final = \" \".join(clean_list)\n",
    "    return cleaned_up_final\n",
    "\n",
    "\n",
    "# remove ellipsis ( why was this so difficult!!!!)\n",
    "def punctuation_clean_up(spec_sents):\n",
    "    for i in range(len(spec_sents)):\n",
    "        spec_sents[i].translate(str.maketrans('', '', string.punctuation))\n",
    "        spec_sents[i] = spec_sents[i].replace('\\u2026', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('”', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('“', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('–', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('!', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('.', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('?', '')\n",
    "        spec_sents[i] = spec_sents[i].replace(',', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('\\n', ' ')\n",
    "        spec_sents[i] = spec_sents[i].replace(\"'\", '')\n",
    "        spec_sents[i] = spec_sents[i].replace(\"’\", '')        \n",
    "\n",
    "    return spec_sents\n",
    "\n",
    "def gensim_preprocess(transcript):\n",
    "    # remove stop words here using gensim, which doesn't require tokenization first\n",
    "    print('pre clean length: ', len(transcript))\n",
    "    \n",
    "    transcript = init_clean(transcript)\n",
    "    print('after init clean: ', len(transcript))\n",
    "\n",
    "    transcript = remove_stopwords(transcript)\n",
    "    print('after stop word removal: ', len(transcript))\n",
    "\n",
    "    transcript = gensim.parsing.preprocessing.strip_numeric(transcript)\n",
    "    print('after removing numbers: ', len(transcript))\n",
    "\n",
    "    transcript = gensim.parsing.preprocessing.strip_short(transcript, minsize=3)\n",
    "    print('after stripping short sentences: ', len(transcript))\n",
    "\n",
    "    \n",
    "    #sentence tokenizing BEFORE stripping punctuation\n",
    "    transcript_sentences = sent_tokenize(transcript)\n",
    "\n",
    "    # clean up all punctuation, replace apostrophes with blank space\n",
    "    transcript_sentences = punctuation_clean_up(transcript_sentences)\n",
    "    \n",
    "    t_sents = transcript_sentences\n",
    "    for i in range(len(transcript_sentences)):\n",
    "        t_sents[i] = word_tokenize(transcript_sentences[i])\n",
    "        \n",
    "    return t_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2sent_vec(w2v, special_sentences):\n",
    "    \n",
    "    no_of_sentences = len(special_sentences)\n",
    "    word_vec_dim = len(w2v['word'])\n",
    "    sentence_vec = np.zeros(word_vec_dim, dtype=np.float32)\n",
    "    sentence_vec = list(np.zeros(no_of_sentences))\n",
    "    errors = 0\n",
    "    for i in range(no_of_sentences):\n",
    "        no_of_words = 0\n",
    "        sent_vec = np.zeros(word_vec_dim, dtype=np.float32)\n",
    "    \n",
    "        for j in range(len(special_sentences[i])):\n",
    "            try:\n",
    "                w2v_sents = w2v[special_sentences[i][j]]\n",
    "                no_of_words += 1\n",
    "                sent_vec += w2v_sents\n",
    "            except:\n",
    "                errors += 1\n",
    "                pass\n",
    "        if no_of_words == 0:\n",
    "            ind_of_err = i\n",
    "            \n",
    "        else:\n",
    "            sent_vec = sent_vec/no_of_words\n",
    "            sentence_vec[i] = sent_vec\n",
    "            \n",
    "    print('errors: ', errors)\n",
    "    \n",
    "    # trim out error indices from zero word sentences\n",
    "    #sentence_vec = np.delete(sentence_vec, no_of_words)\n",
    "    \n",
    "    # routine that checks for any zero'd elements and removes them\n",
    "    print('shape before: ', np.shape(sentence_vec))\n",
    "    \n",
    "    err_flag = 0\n",
    "    err_list = []\n",
    "    for i in range(no_of_sentences):\n",
    "        if np.shape(sentence_vec[i]) == ():\n",
    "            err_flag = 1\n",
    "            err_list.append(i)\n",
    "    \n",
    "    if err_flag == 1:\n",
    "        err_array = np.asarray(err_list, dtype=int)\n",
    "        sentence_vec_fix = np.delete(sentence_vec, err_array)\n",
    "        return sentence_vec_fix\n",
    "    else:\n",
    "        return sentence_vec\n",
    "\n",
    "def cos_sim(sent1, sent2):\n",
    "    norm = np.linalg.norm(sent1) * np.linalg.norm(sent2)\n",
    "    cosine_similarity = np.dot(sent1, sent2) / norm\n",
    "    return cosine_similarity\n",
    "\n",
    "def sentence_sim_dist(sent_vecs, chunk_size):\n",
    "\n",
    "    sim_dist = np.zeros(chunk_size)\n",
    "    end_index = len(sent_vecs)\n",
    "    # scan through all sentences!\n",
    "\n",
    "    for i in range(end_index - chunk_size):\n",
    "        sent_mid = sent_vecs[i]\n",
    "        #print('i: ',i)\n",
    "        for j in range(chunk_size):\n",
    "            k = i + j\n",
    "            #print('j: ', j, ' k: ', k)\n",
    "            #print('cos_sim: ', cos_sim(sent_mid, sent_vecs[k]))\n",
    "            sim_dist[j] += cos_sim(sent_mid, sent_vecs[k])\n",
    "    norm = sim_dist[0]\n",
    "    sim_dist = sim_dist/norm\n",
    "    return sim_dist\n",
    "\n",
    "def shuffler(sentence_vecs):\n",
    "    sents_total = len(sentence_vecs)\n",
    "    sent_dim = len(sentence_vecs[0])\n",
    "    sentence_shuffle = np.zeros((sents_total, sent_dim), dtype=np.float32)\n",
    "    rand_index = np.arange(sents_total)\n",
    "    np.random.shuffle(rand_index)\n",
    "    for i in range(sents_total):\n",
    "        j = rand_index[i]\n",
    "        sentence_shuffle[j] = sentence_vecs[i]\n",
    "    return sentence_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_data = '/Users/johnpapaioannou/Desktop/insight/project/nlp_models/full_transcripts.pkl'\n",
    "specials = pd.read_pickle(pickled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of specials:  126\n",
      "total:  11627863 mean:  92284.62698412698\n",
      "pre clean length:  94514\n",
      "after init clean:  92429\n",
      "after stop word removal:  62361\n",
      "after removing numbers:  62262\n",
      "after stripping short sentences:  59809\n",
      "# of sentences:  1785\n",
      "errors:  135\n",
      "shape before:  (1785, 300)\n",
      "125 1.0\n",
      "125 0.4636962954608925\n",
      "125 0.4192412549878019\n",
      "125 0.4101665923067706\n",
      "125 0.4031886906338893\n",
      "125 0.40059968823597947\n",
      "125 0.3971186820667222\n",
      "125 0.3919828402867171\n",
      "125 0.3892525735265025\n",
      "125 0.3926952057423681\n",
      "125 0.3906431623097245\n",
      "125 0.38940696302675465\n",
      "baseline similarity:  0.37751080604846515\n"
     ]
    }
   ],
   "source": [
    "no_of_specials = len(specials.iloc[:,1])\n",
    "print('# of specials: ', no_of_specials)\n",
    "\n",
    "# for i in range(no_of_specials):\n",
    "#     print('i: ', i, 'comedian: ', specials.iloc[i,0])\n",
    "    \n",
    "jeselnik = specials.iloc[7,1]\n",
    "birbigs = specials.iloc[89,1]\n",
    "hedberg = specials.iloc[91,1]\n",
    "patrice = specials.iloc[99,1]\n",
    "bargatze = specials.iloc[93,1]\n",
    "maron = specials.iloc[82,1]\n",
    "carr = specials.iloc[65,1]\n",
    "demetri_martin = specials.iloc[34,1]\n",
    "amy_schumer = specials.iloc[5,1]\n",
    "colin_quinn = specials.iloc[24,1]\n",
    "kevin_smith = specials.iloc[77,1]\n",
    "stewart_lee = specials.iloc[117,1]\n",
    "attell = specials.iloc[31,1]\n",
    "\n",
    "char_tot = 0\n",
    "for i in range(no_of_specials):\n",
    "    char_tot += len(specials.iloc[i,1])\n",
    "\n",
    "print('total: ', char_tot, 'mean: ', char_tot/no_of_specials)\n",
    "\n",
    "special_sentences = gensim_preprocess(specials.iloc[7,1])\n",
    "\n",
    "print('# of sentences: ', len(special_sentences))\n",
    "\n",
    "#print(special_sentences)\n",
    "sentence_vecs = word2sent_vec(w2v_model, special_sentences)\n",
    "chunk_size = 12\n",
    "\n",
    "sim_dist = sentence_sim_dist(sentence_vecs, chunk_size)\n",
    "for j in range(chunk_size):\n",
    "    print(i, sim_dist[j])\n",
    "\n",
    "sentence_shuffle = shuffler(sentence_vecs)\n",
    "sim_dist_shuffle = sentence_sim_dist(sentence_shuffle, chunk_size)\n",
    "print('baseline similarity: ', np.mean(sim_dist_shuffle[1:-1]))\n",
    "\n",
    "baseline_sim = np.mean(sim_dist_shuffle[1:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
