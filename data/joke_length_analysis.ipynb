{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import everything\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS, remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "from profanity_check import predict, predict_prob\n",
    "from itertools import compress\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in pre-trained word2vec model\n",
    "import gensim.downloader as api\n",
    "w2v_path = '~/gensim-data/word2vec-google-news-300'\n",
    "w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection of annoying punctuation specific to these comedy transcripts\n",
    "def init_clean(single_special):\n",
    "    single_special = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", single_special)\n",
    "    single_special = re.sub(\"-\", \" \", single_special)\n",
    "    single_special = re.sub(\"♪\", \"\", single_special)\n",
    "    return single_special\n",
    "\n",
    "# filter out sentences that are 2 or less words!\n",
    "def short_sents_filter(text):\n",
    "    for sentence in text:\n",
    "        words = sentence.split(\" \")\n",
    "        if len(words) < 3:\n",
    "            text.remove(sentence)\n",
    "    return text\n",
    "\n",
    "\n",
    "# lemmatizing\n",
    "stemmer = SnowballStemmer('english')\n",
    "#comedy_cliche = ['like', 'know', 'say', 'look', 'come', 'right', 'go', 'think']\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            #if token not in comedy_cliche:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def profane_filter(text):\n",
    "    cleaned_up = predict(text)\n",
    "    cleaned_up = 1 - cleaned_up\n",
    "    clean_list = list(compress(text, cleaned_up))\n",
    "    cleaned_up_final = \" \".join(clean_list)\n",
    "    return cleaned_up_final\n",
    "\n",
    "\n",
    "# remove ellipsis ( why was this so difficult!!!!)\n",
    "def punctuation_clean_up(spec_sents):\n",
    "    for i in range(len(spec_sents)):\n",
    "        spec_sents[i].translate(str.maketrans('', '', string.punctuation))\n",
    "        spec_sents[i] = spec_sents[i].replace('\\u2026', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('”', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('“', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('–', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('!', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('.', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('?', '')\n",
    "        spec_sents[i] = spec_sents[i].replace(',', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('\\n', ' ')\n",
    "        spec_sents[i] = spec_sents[i].replace(\"'\", '')\n",
    "        spec_sents[i] = spec_sents[i].replace(\"’\", '')        \n",
    "\n",
    "    return spec_sents\n",
    "\n",
    "def gensim_preprocess(transcript):\n",
    "    # remove stop words here using gensim, which doesn't require tokenization first\n",
    "    print('pre clean length: ', len(transcript))\n",
    "    \n",
    "    transcript = init_clean(transcript)\n",
    "    print('after init clean: ', len(transcript))\n",
    "\n",
    "    transcript = remove_stopwords(transcript)\n",
    "    print('after stop word removal: ', len(transcript))\n",
    "\n",
    "    transcript = gensim.parsing.preprocessing.strip_numeric(transcript)\n",
    "    print('after removing numbers: ', len(transcript))\n",
    "\n",
    "    transcript = gensim.parsing.preprocessing.strip_short(transcript, minsize=3)\n",
    "    print('after stripping short sentences: ', len(transcript))\n",
    "\n",
    "    \n",
    "    #sentence tokenizing BEFORE stripping punctuation\n",
    "    transcript_sentences = sent_tokenize(transcript)\n",
    "\n",
    "    # clean up all punctuation, replace apostrophes with blank space\n",
    "    transcript_sentences = punctuation_clean_up(transcript_sentences)\n",
    "    \n",
    "    t_sents = transcript_sentences\n",
    "    for i in range(len(transcript_sentences)):\n",
    "        t_sents[i] = word_tokenize(transcript_sentences[i])\n",
    "        \n",
    "    return t_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2sent_vec(w2v, special_sentences):\n",
    "    \n",
    "    no_of_sentences = len(special_sentences)\n",
    "    word_vec_dim = len(w2v['word'])\n",
    "    sentence_vec = np.zeros(word_vec_dim, dtype=np.float32)\n",
    "    sentence_vec = list(np.zeros(no_of_sentences))\n",
    "    errors = 0\n",
    "    for i in range(no_of_sentences):\n",
    "        no_of_words = 0\n",
    "        sent_vec = np.zeros(word_vec_dim, dtype=np.float32)\n",
    "    \n",
    "        for j in range(len(special_sentences[i])):\n",
    "            try:\n",
    "                w2v_sents = w2v[special_sentences[i][j]]\n",
    "                no_of_words += 1\n",
    "                sent_vec += w2v_sents\n",
    "            except:\n",
    "                errors += 1\n",
    "                pass\n",
    "        if no_of_words == 0:\n",
    "            ind_of_err = i\n",
    "            \n",
    "        else:\n",
    "            sent_vec = sent_vec/no_of_words\n",
    "            sentence_vec[i] = sent_vec\n",
    "            \n",
    "    print('errors: ', errors)\n",
    "    \n",
    "    # trim out error indices from zero word sentences\n",
    "    #sentence_vec = np.delete(sentence_vec, no_of_words)\n",
    "    \n",
    "    # routine that checks for any zero'd elements and removes them\n",
    "    print('shape before: ', np.shape(sentence_vec))\n",
    "    \n",
    "    err_flag = 0\n",
    "    err_list = []\n",
    "    for i in range(no_of_sentences):\n",
    "        if np.shape(sentence_vec[i]) == ():\n",
    "            err_flag = 1\n",
    "            err_list.append(i)\n",
    "    \n",
    "    if err_flag == 1:\n",
    "        err_array = np.asarray(err_list, dtype=int)\n",
    "        sentence_vec_fix = np.delete(sentence_vec, err_array)\n",
    "        return sentence_vec_fix\n",
    "    else:\n",
    "        return sentence_vec\n",
    "\n",
    "def cos_sim(sent1, sent2):\n",
    "    norm = np.linalg.norm(sent1) * np.linalg.norm(sent2)\n",
    "    cosine_similarity = np.dot(sent1, sent2) / norm\n",
    "    return cosine_similarity\n",
    "\n",
    "def sentence_sim_dist(sent_vecs, chunk_size):\n",
    "\n",
    "    sim_dist = np.zeros(chunk_size)\n",
    "    end_index = len(sent_vecs)\n",
    "    # scan through all sentences!\n",
    "\n",
    "    for i in range(end_index - chunk_size):\n",
    "        sent_mid = sent_vecs[i]\n",
    "        #print('i: ',i)\n",
    "        for j in range(chunk_size):\n",
    "            k = i + j\n",
    "            #print('j: ', j, ' k: ', k)\n",
    "            #print('cos_sim: ', cos_sim(sent_mid, sent_vecs[k]))\n",
    "            sim_dist[j] += cos_sim(sent_mid, sent_vecs[k])\n",
    "    norm = sim_dist[0]\n",
    "    sim_dist = sim_dist/norm\n",
    "    return sim_dist\n",
    "\n",
    "import math\n",
    "# check for NaN's in w2v embedding, meaning I didn't catch something in preprocessing\n",
    "# def check_for_nans(sentence_vecs):\n",
    "#     for i in range(len(sentence_vecs)):\n",
    "#         for j in range(np.shape(sentence_vecs[0])[0]):\n",
    "#             if math.isnan(sentence_vecs[i][j]):\n",
    "#                 #print('i: ', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of specials:  126\n",
      "i:  0 comedian:  adam devine\n",
      "i:  1 comedian:  adam sandler\n",
      "i:  2 comedian:  adel karam\n",
      "i:  3 comedian:  al madrigal\n",
      "i:  4 comedian:  ali wong\n",
      "i:  5 comedian:  amy schumer\n",
      "i:  6 comedian:  anjelah johnson\n",
      "i:  7 comedian:  anthony jeselnik\n",
      "i:  8 comedian:  ari shaffir\n",
      "i:  9 comedian:  aziz ansari\n",
      "i:  10 comedian:  bert kreischer\n",
      "i:  11 comedian:  big jay oakerson\n",
      "i:  12 comedian:  bill burr\n",
      "i:  13 comedian:  bill hicks\n",
      "i:  14 comedian:  bill maher\n",
      "i:  15 comedian:  bo burnham\n",
      "i:  16 comedian:  brad williams\n",
      "i:  17 comedian:  brent morin\n",
      "i:  18 comedian:  brian regan\n",
      "i:  19 comedian:  bridget everett\n",
      "i:  20 comedian:  cedric the entertainer\n",
      "i:  21 comedian:  chelsea peretti\n",
      "i:  22 comedian:  chris rock\n",
      "i:  23 comedian:  chris tucker\n",
      "i:  24 comedian:  colin quinn\n",
      "i:  25 comedian:  craig ferguson\n",
      "i:  26 comedian:  cristela alonzo\n",
      "i:  27 comedian:  d l hughley\n",
      "i:  28 comedian:  dana carvey\n",
      "i:  29 comedian:  daniel sloss\n",
      "i:  30 comedian:  daniel tosh\n",
      "i:  31 comedian:  dave attell\n",
      "i:  32 comedian:  dave chappelle\n",
      "i:  33 comedian:  david cross\n",
      "i:  34 comedian:  demetri martin\n",
      "i:  35 comedian:  deray davis\n",
      "i:  36 comedian:  donald glover\n",
      "i:  37 comedian:  doug stanhope\n",
      "i:  38 comedian:  dylan moran\n",
      "i:  39 comedian:  eddie griffin\n",
      "i:  40 comedian:  eddie izzard\n",
      "i:  41 comedian:  eddie murphy\n",
      "i:  42 comedian:  ellen degeneres\n",
      "i:  43 comedian:  emily heller\n",
      "i:  44 comedian:  enissa amani\n",
      "i:  45 comedian:  erik griffin\n",
      "i:  46 comedian:  fahim anwar\n",
      "i:  47 comedian:  frankie boyle\n",
      "i:  48 comedian:  fred armisen\n",
      "i:  49 comedian:  gabriel iglesias\n",
      "i:  50 comedian:  george carlin\n",
      "i:  51 comedian:  gina yashere\n",
      "i:  52 comedian:  greg davies\n",
      "i:  53 comedian:  hannah gadsby\n",
      "i:  54 comedian:  hannibal buress\n",
      "i:  55 comedian:  hasan minhaj\n",
      "i:  56 comedian:  henry rollins\n",
      "i:  57 comedian:  iliza shlesinger\n",
      "i:  58 comedian:  jack whitehall\n",
      "i:  59 comedian:  jeff foxworthy\n",
      "i:  60 comedian:  jen kirkman\n",
      "i:  61 comedian:  jerry seinfeld\n",
      "i:  62 comedian:  jim gaffigan\n",
      "i:  63 comedian:  jim jefferies\n",
      "i:  64 comedian:  jim norton\n",
      "i:  65 comedian:  jimmy carr\n",
      "i:  66 comedian:  jo koy\n",
      "i:  67 comedian:  joe mandes\n",
      "i:  68 comedian:  joe rogan\n",
      "i:  69 comedian:  john leguizamo\n",
      "i:  70 comedian:  john mulaney\n",
      "i:  71 comedian:  judah friedlander\n",
      "i:  72 comedian:  katt williams\n",
      "i:  73 comedian:  kavin jay\n",
      "i:  74 comedian:  ken jeong\n",
      "i:  75 comedian:  kevin hart\n",
      "i:  76 comedian:  kevin james\n",
      "i:  77 comedian:  kevin smith\n",
      "i:  78 comedian:  lenny bruce\n",
      "i:  79 comedian:  lewis black\n",
      "i:  80 comedian:  lisa lampanelli\n",
      "i:  81 comedian:  louis ck\n",
      "i:  82 comedian:  marc maron\n",
      "i:  83 comedian:  maria bamford\n",
      "i:  84 comedian:  marlon wayans\n",
      "i:  85 comedian:  maz jobrani\n",
      "i:  86 comedian:  michael che\n",
      "i:  87 comedian:  michael mcintyre\n",
      "i:  88 comedian:  michelle wolf\n",
      "i:  89 comedian:  mike birbiglia\n",
      "i:  90 comedian:  mike epps\n",
      "i:  91 comedian:  mitch hedberg\n",
      "i:  92 comedian:  mo amer\n",
      "i:  93 comedian:  nate bargatze\n",
      "i:  94 comedian:  neal brennan\n",
      "i:  95 comedian:  nick offerman\n",
      "i:  96 comedian:  nikki glaser\n",
      "i:  97 comedian:  norm macdonald\n",
      "i:  98 comedian:  pablo francisco\n",
      "i:  99 comedian:  patrice oneal\n",
      "i:  100 comedian:  patton oswalt\n",
      "i:  101 comedian:  paul mooney\n",
      "i:  102 comedian:  pete holmes\n",
      "i:  103 comedian:  ralphie may\n",
      "i:  104 comedian:  ray romano\n",
      "i:  105 comedian:  richard pryor\n",
      "i:  106 comedian:  ricky gervais\n",
      "i:  107 comedian:  robin williams\n",
      "i:  108 comedian:  ron white\n",
      "i:  109 comedian:  rowan atkinson\n",
      "i:  110 comedian:  roy wood\n",
      "i:  111 comedian:  russell brand\n",
      "i:  112 comedian:  russell howard\n",
      "i:  113 comedian:  russell peters\n",
      "i:  114 comedian:  sarah millican\n",
      "i:  115 comedian:  sarah silverman\n",
      "i:  116 comedian:  sebastian maniscalco\n",
      "i:  117 comedian:  stewart lee\n",
      "i:  118 comedian:  todd barry\n",
      "i:  119 comedian:  todd glass\n",
      "i:  120 comedian:  tom segura\n",
      "i:  121 comedian:  trevor noah\n",
      "i:  122 comedian:  vir das\n",
      "i:  123 comedian:  volker pispers\n",
      "i:  124 comedian:  wanda sykes\n",
      "i:  125 comedian:  whitney cummings\n",
      "pre clean length:  50450\n",
      "after init clean:  50362\n",
      "after stop word removal:  35930\n",
      "after removing numbers:  35881\n",
      "after stripping short sentences:  34361\n",
      "# of sentences:  770\n",
      "errors:  90\n",
      "shape before:  (770, 300)\n",
      "125 1.0\n",
      "125 0.5072681610120339\n",
      "125 0.4807524882341794\n",
      "125 0.47673224630433814\n",
      "125 0.46592142472102316\n",
      "125 0.4641488836624104\n"
     ]
    }
   ],
   "source": [
    "pickled_data = '/Users/johnpapaioannou/Desktop/insight/project/nlp_models/full_transcripts.pkl'\n",
    "specials = pd.read_pickle(pickled_data)\n",
    "\n",
    "no_of_specials = len(specials.iloc[:,1])\n",
    "\n",
    "print('# of specials: ', no_of_specials)\n",
    "\n",
    "for i in range(no_of_specials):\n",
    "    print('i: ', i, 'comedian: ', specials.iloc[i,0])\n",
    "    \n",
    "jeselnik = specials.iloc[7,1]\n",
    "birbigs = specials.iloc[89,1]\n",
    "hedberg = specials.iloc[91,1]\n",
    "patrice = specials.iloc[99,1]\n",
    "bargatze = specials.iloc[93,1]\n",
    "maron = specials.iloc[82,1]\n",
    "carr = specials.iloc[65,1]\n",
    "demetri_martin = specials.iloc[34,1]\n",
    "amy_schumer = specials.iloc[5,1]\n",
    "colin_quinn = specials.iloc[24,1]\n",
    "kevin_smith = specials.iloc[77,1]\n",
    "stewart_lee = specials.iloc[117,1]\n",
    "attell = specials.iloc[31,1]\n",
    "\n",
    "special_sentences = gensim_preprocess(specials.iloc[119,1])\n",
    "\n",
    "print('# of sentences: ', len(special_sentences))\n",
    "\n",
    "#print(special_sentences)\n",
    "sentence_vecs = word2sent_vec(w2v_model, special_sentences)\n",
    "chunk_size = 6\n",
    "\n",
    "sim_dist = sentence_sim_dist(sentence_vecs, chunk_size)\n",
    "for j in range(chunk_size):\n",
    "    print(i, sim_dist[j])\n",
    "    \n",
    "\n",
    "# print(np.shape(sentence_vecs))\n",
    "# print(sentence_vecs[125])\n",
    "# chunk_size = 8\n",
    "# sim_dist = sentence_sim_dist(sentence_vecs, chunk_size)\n",
    "# for j in range(2*chunk_size + 1):\n",
    "#     print(i, sim_dist[j])\n",
    "\n",
    "# chunk_size = 8\n",
    "# sim_dist = sentence_sim_dist(sentence_vecs, chunk_size)\n",
    "# for j in range(2*chunk_size + 1):\n",
    "#     print(i, sim_dist[j])\n",
    "\n",
    "# print('\\nMaron')\n",
    "# special_sentences = gensim_preprocess(specials.iloc[82,1])\n",
    "# sentence_vecs = word2sent_vec(w2v_model, special_sentences)\n",
    "\n",
    "# chunk_size = 8\n",
    "# sim_dist = sentence_sim_dist(sentence_vecs, chunk_size)\n",
    "# for j in range(2*chunk_size + 1):\n",
    "#     print(i, sim_dist[j])\n",
    "    \n",
    "# print('Jeselnik')\n",
    "# special_sentences = gensim_preprocess(specials.iloc[7,1])\n",
    "# sentence_vecs = word2sent_vec(w2v_model, special_sentences)\n",
    "\n",
    "# chunk_size = 8\n",
    "# sim_dist = sentence_sim_dist(sentence_vecs, chunk_size)\n",
    "# for j in range(2*chunk_size + 1):\n",
    "#     print(i, sim_dist[j])\n",
    "\n",
    "# chunk_size = 8\n",
    "# sim_dist = sentence_sim_dist(sentence_vecs, chunk_size)\n",
    "# for j in range(2*chunk_size + 1):\n",
    "#     print(i, sim_dist[j])\n",
    "\n",
    "# print('\\nCarr')\n",
    "# special_sentences = gensim_preprocess(specials.iloc[65,1])\n",
    "# sentence_vecs = word2sent_vec(w2v_model, special_sentences)\n",
    "\n",
    "# chunk_size = 8\n",
    "# sim_dist = sentence_sim_dist(sentence_vecs, chunk_size)\n",
    "# for j in range(2*chunk_size + 1):\n",
    "#     print(i, sim_dist[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre clean length:  47948\n",
      "after init clean:  44497\n",
      "after stop word removal:  32527\n",
      "after removing numbers:  32430\n",
      "after stripping short sentences:  31461\n",
      "errors:  628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in float_scalars\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-c0c373ba8743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mchunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msim_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_sim_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mchunk_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-48ca6bd17542>\u001b[0m in \u001b[0;36msentence_sim_dist\u001b[0;34m(sent_vecs, chunk_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_dist_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0msim_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0msim_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim_dist\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-48ca6bd17542>\u001b[0m in \u001b[0;36mcos_sim\u001b[0;34m(sent1, sent2)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mcosine_similarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2439\u001b[0m     \u001b[0;31m# Immediately handle some default, simple, fast, and common cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2440\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2441\u001b[0;31m         \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2442\u001b[0m         if ((ord is None) or\n\u001b[1;32m   2443\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'f'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fro'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    special_sentences = gensim_preprocess(specials.iloc[i,1])\n",
    "    sentence_vecs = word2sent_vec(w2v_model, special_sentences)\n",
    "\n",
    "    chunk_size = 8\n",
    "    sim_dist = sentence_sim_dist(sentence_vecs, chunk_size)\n",
    "    for j in range(2*chunk_size + 1):\n",
    "        print(i, sim_dist[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
