{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import everything\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS, remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "from profanity_check import predict, predict_prob\n",
    "from itertools import compress\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in pre-trained word2vec model\n",
    "import gensim.downloader as api\n",
    "w2v_path = '~/gensim-data/word2vec-google-news-300'\n",
    "w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection of annoying punctuation specific to these comedy transcripts\n",
    "def init_clean(single_special):\n",
    "    single_special = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", single_special)\n",
    "    single_special = re.sub(\"-\", \" \", single_special)\n",
    "    single_special = re.sub(\"♪\", \"\", single_special)\n",
    "    return single_special\n",
    "\n",
    "# filter out sentences that are 2 or less words!\n",
    "def short_sents_filter(text):\n",
    "    for sentence in text:\n",
    "        words = sentence.split(\" \")\n",
    "        if len(words) < 3:\n",
    "            text.remove(sentence)\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            #if token not in comedy_cliche:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def profane_filter(text):\n",
    "    cleaned_up = predict(text)\n",
    "    cleaned_up = 1 - cleaned_up\n",
    "    clean_list = list(compress(text, cleaned_up))\n",
    "    cleaned_up_final = \" \".join(clean_list)\n",
    "    return cleaned_up_final\n",
    "\n",
    "\n",
    "# remove ellipsis ( why was this so difficult!!!!)\n",
    "def punctuation_clean_up(spec_sents):\n",
    "    for i in range(len(spec_sents)):\n",
    "        spec_sents[i].translate(str.maketrans('', '', string.punctuation))\n",
    "        spec_sents[i] = spec_sents[i].replace('\\u2026', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('”', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('“', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('–', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('!', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('.', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('?', '')\n",
    "        spec_sents[i] = spec_sents[i].replace(',', '')\n",
    "        spec_sents[i] = spec_sents[i].replace('\\n', ' ')\n",
    "        spec_sents[i] = spec_sents[i].replace(\"'\", '')\n",
    "        spec_sents[i] = spec_sents[i].replace(\"’\", '')        \n",
    "\n",
    "    return spec_sents\n",
    "\n",
    "def gensim_preprocess(transcript):\n",
    "    # remove stop words here using gensim, which doesn't require tokenization first\n",
    "    #print('pre clean length: ', len(transcript))\n",
    "    \n",
    "    transcript = init_clean(transcript)\n",
    "    #print('after init clean: ', len(transcript))\n",
    "\n",
    "    transcript = remove_stopwords(transcript)\n",
    "    #print('after stop word removal: ', len(transcript))\n",
    "\n",
    "    transcript = gensim.parsing.preprocessing.strip_numeric(transcript)\n",
    "    #print('after removing numbers: ', len(transcript))\n",
    "\n",
    "    transcript = gensim.parsing.preprocessing.strip_short(transcript, minsize=3)\n",
    "    #print('after stripping short sentences: ', len(transcript))\n",
    "\n",
    "    \n",
    "    #sentence tokenizing BEFORE stripping punctuation\n",
    "    transcript_sentences = sent_tokenize(transcript)\n",
    "\n",
    "    # clean up all punctuation, replace apostrophes with blank space\n",
    "    transcript_sentences = punctuation_clean_up(transcript_sentences)\n",
    "    \n",
    "    t_sents = transcript_sentences\n",
    "    for i in range(len(transcript_sentences)):\n",
    "        t_sents[i] = word_tokenize(transcript_sentences[i])\n",
    "        \n",
    "    return t_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2sent_vec takes in transcripts parsed into sentences and returns all sentence vectors\n",
    "\n",
    "# sentence_sim_dist takes in the sentences vectors and how far ahead in sentences you want to include in the autocorrelation and returns the similarity autocorrelation function\n",
    "\n",
    "# shuffler randomly shuffles the order of sentence vectors in a transcript for calculating each comedian's baseline rate of average sentence to sentence similarity in order to compare different comedians' similarity autocorrelation functions with one another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2sent_vec(w2v, special_sentences):\n",
    "    \n",
    "    no_of_sentences = len(special_sentences)\n",
    "    word_vec_dim = len(w2v['word'])\n",
    "    sentence_vec = np.zeros(word_vec_dim, dtype=np.float32)\n",
    "    sentence_vec = list(np.zeros(no_of_sentences))\n",
    "    errors = 0\n",
    "    for i in range(no_of_sentences):\n",
    "        no_of_words = 0\n",
    "        sent_vec = np.zeros(word_vec_dim, dtype=np.float32)\n",
    "    \n",
    "        for j in range(len(special_sentences[i])):\n",
    "            try:\n",
    "                w2v_sents = w2v[special_sentences[i][j]]\n",
    "                no_of_words += 1\n",
    "                sent_vec += w2v_sents\n",
    "            except:\n",
    "                errors += 1\n",
    "                pass\n",
    "        if no_of_words == 0:\n",
    "            ind_of_err = i\n",
    "            \n",
    "        else:\n",
    "            sent_vec = sent_vec/no_of_words\n",
    "            sentence_vec[i] = sent_vec\n",
    "            \n",
    "    print('errors: ', errors)\n",
    "    \n",
    "    # trim out error indices from zero word sentences\n",
    "    #sentence_vec = np.delete(sentence_vec, no_of_words)\n",
    "    \n",
    "    # routine that checks for any zero'd elements and removes them\n",
    "    #print('shape before: ', np.shape(sentence_vec))\n",
    "    \n",
    "    err_flag = 0\n",
    "    err_list = []\n",
    "    for i in range(no_of_sentences):\n",
    "        if np.shape(sentence_vec[i]) == ():\n",
    "            err_flag = 1\n",
    "            err_list.append(i)\n",
    "    \n",
    "    if err_flag == 1:\n",
    "        err_array = np.asarray(err_list, dtype=int)\n",
    "        sentence_vec_fix = np.delete(sentence_vec, err_array)\n",
    "        return sentence_vec_fix\n",
    "    else:\n",
    "        return sentence_vec\n",
    "\n",
    "def cos_sim(sent1, sent2):\n",
    "    norm = np.linalg.norm(sent1) * np.linalg.norm(sent2)\n",
    "    cosine_similarity = np.dot(sent1, sent2) / norm\n",
    "    return cosine_similarity\n",
    "\n",
    "def sentence_sim_dist(sent_vecs, chunk_size):\n",
    "\n",
    "    sim_dist = np.zeros(chunk_size)\n",
    "    end_index = len(sent_vecs)\n",
    "    # scan through all sentences!\n",
    "\n",
    "    for i in range(end_index - chunk_size):\n",
    "        sent_mid = sent_vecs[i]\n",
    "        #print('i: ',i)\n",
    "        for j in range(chunk_size):\n",
    "            k = i + j\n",
    "            #print('j: ', j, ' k: ', k)\n",
    "            #print('cos_sim: ', cos_sim(sent_mid, sent_vecs[k]))\n",
    "            sim_dist[j] += cos_sim(sent_mid, sent_vecs[k])\n",
    "    norm = sim_dist[0]\n",
    "    sim_dist = sim_dist/norm\n",
    "    return sim_dist\n",
    "\n",
    "def shuffler(sentence_vecs):\n",
    "    sents_total = len(sentence_vecs)\n",
    "    sent_dim = len(sentence_vecs[0])\n",
    "    sentence_shuffle = np.zeros((sents_total, sent_dim), dtype=np.float32)\n",
    "    rand_index = np.arange(sents_total)\n",
    "    np.random.shuffle(rand_index)\n",
    "    for i in range(sents_total):\n",
    "        j = rand_index[i]\n",
    "        sentence_shuffle[j] = sentence_vecs[i]\n",
    "    return sentence_shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full routine that takes in a transcript, preprocesses, cleans, parses into sentences, constructs sentence vecs, calculates the average similarity in sentences and returns the baseline adjusted sentence similarity autocorrelation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joke_length(special):\n",
    "    \n",
    "    chunk_size = 20\n",
    "    special_sentences = gensim_preprocess(special)\n",
    "    sentence_vecs = word2sent_vec(w2v_model, special_sentences)\n",
    "    sim_dist = sentence_sim_dist(sentence_vecs, chunk_size)\n",
    "\n",
    "    # ensemble average shuffled sentences to get a low variance baseline sentence2sentence similarity\n",
    "    sim_dist_shuffle = np.zeros(chunk_size)\n",
    "    ensemble_size = 10\n",
    "    for i in range(ensemble_size):\n",
    "        sentence_shuffle = shuffler(sentence_vecs)\n",
    "        sim_dist_shuffle += sentence_sim_dist(sentence_shuffle, chunk_size)\n",
    "    \n",
    "\n",
    "    baseline_ensembled = np.mean(sim_dist_shuffle[1:])/ensemble_size\n",
    "    print('baseline similarity: ', baseline_ensembled)\n",
    "    baseline_sim = np.mean(sim_dist_shuffle[1:])\n",
    "\n",
    "    sim_dist_adjusted = sim_dist - baseline_ensembled\n",
    "    \n",
    "    return sim_dist_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       comedian                                         transcript\n",
      "0   adam devine  \\n[rock music playing]\\n[indistinct chatter]\\n...\n",
      "1  adam sandler  \\n[man] Okay, ready, and… Take your own cue, A...\n",
      "2    adel karam  \\nA NETFLIX COMEDY SPECIAL\\nRecorded at the Ca...\n",
      "3   al madrigal  \\n[dog barks] [FisherGreen’s Sisters Brothers ...\n",
      "4      ali wong  \\nLadies and gentlemen, please welcome to the ...\n"
     ]
    }
   ],
   "source": [
    "# read in full trancripts\n",
    "pickled_data = '/Users/johnpapaioannou/Desktop/insight/project/data/full_transcripts.pkl'\n",
    "specials = pd.read_pickle(pickled_data)\n",
    "print(specials.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map joke_length to all transcripts, getting all sentence similarity autocorrelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors:  85\n",
      "baseline similarity:  0.4321867470889216\n",
      "errors:  68\n",
      "baseline similarity:  0.4595031502493091\n",
      "errors:  47\n",
      "baseline similarity:  0.31274515826852833\n",
      "errors:  80\n",
      "baseline similarity:  0.3614336887357196\n",
      "errors:  91\n",
      "baseline similarity:  0.39763033090436467\n",
      "errors:  281\n",
      "baseline similarity:  0.42199390437846285\n",
      "errors:  109\n",
      "baseline similarity:  0.38326064083747025\n",
      "errors:  135\n",
      "baseline similarity:  0.3777183739568772\n",
      "errors:  190\n",
      "baseline similarity:  0.38455763365309686\n",
      "errors:  320\n",
      "baseline similarity:  0.40877092403278936\n",
      "errors:  74\n",
      "baseline similarity:  0.4175643903299127\n",
      "errors:  80\n",
      "baseline similarity:  0.39874296973000073\n",
      "errors:  446\n",
      "baseline similarity:  0.41843464792935514\n",
      "errors:  362\n",
      "baseline similarity:  0.40021487372778236\n",
      "errors:  248\n",
      "baseline similarity:  0.37507915058719493\n",
      "errors:  169\n",
      "baseline similarity:  0.33697362290773797\n",
      "errors:  71\n",
      "baseline similarity:  0.4403948093806025\n",
      "errors:  101\n",
      "baseline similarity:  0.40430057729420055\n",
      "errors:  114\n",
      "baseline similarity:  0.3632046939589121\n",
      "errors:  27\n",
      "baseline similarity:  0.41603532474614846\n",
      "errors:  131\n",
      "baseline similarity:  0.41758541367516955\n",
      "errors:  53\n",
      "baseline similarity:  0.4350805692322905\n",
      "errors:  495\n",
      "baseline similarity:  0.39516300505050483\n",
      "errors:  181\n",
      "baseline similarity:  0.4067579501012193\n",
      "errors:  95\n",
      "baseline similarity:  0.34516139306160065\n",
      "errors:  408\n",
      "baseline similarity:  0.39000593188209615\n",
      "errors:  55\n",
      "baseline similarity:  0.3762685359928394\n",
      "errors:  272\n",
      "baseline similarity:  0.4199864262896197\n",
      "errors:  68\n",
      "baseline similarity:  0.35931470176915864\n",
      "errors:  127\n",
      "baseline similarity:  0.39181830149058083\n",
      "errors:  299\n",
      "baseline similarity:  0.3657985267016555\n",
      "errors:  166\n",
      "baseline similarity:  0.385267423733512\n",
      "errors:  636\n",
      "baseline similarity:  0.38977325246868827\n",
      "errors:  84\n",
      "baseline similarity:  0.41180899235950763\n",
      "errors:  112\n",
      "baseline similarity:  0.39575068080583514\n",
      "errors:  129\n",
      "baseline similarity:  0.38288359149442364\n",
      "errors:  122\n",
      "baseline similarity:  0.4643101740255983\n",
      "errors:  296\n",
      "baseline similarity:  0.3901807277294146\n",
      "errors:  570\n",
      "baseline similarity:  0.45368782977066757\n",
      "errors:  255\n",
      "baseline similarity:  0.4144197341164535\n",
      "errors:  1782\n",
      "baseline similarity:  0.3267802201950536\n",
      "errors:  519\n",
      "baseline similarity:  0.42868686779352394\n",
      "errors:  286\n",
      "baseline similarity:  0.425181878450749\n",
      "errors:  52\n",
      "baseline similarity:  0.4361296107992202\n",
      "errors:  92\n",
      "baseline similarity:  0.3622833708099633\n",
      "errors:  37\n",
      "baseline similarity:  0.3921262545610386\n",
      "errors:  51\n",
      "baseline similarity:  0.4305652339335692\n",
      "errors:  372\n",
      "baseline similarity:  0.3726075811269111\n",
      "errors:  67\n",
      "baseline similarity:  0.3321263425533509\n",
      "errors:  603\n",
      "baseline similarity:  0.37893843419352496\n",
      "errors:  1018\n",
      "baseline similarity:  0.3730936221775302\n",
      "errors:  36\n",
      "baseline similarity:  0.3232661804786949\n",
      "errors:  57\n",
      "baseline similarity:  0.3769220578069484\n",
      "errors:  81\n",
      "baseline similarity:  0.3222538913310629\n",
      "errors:  98\n",
      "baseline similarity:  0.3742635077479647\n",
      "errors:  118\n",
      "baseline similarity:  0.3457083330043832\n",
      "errors:  57\n",
      "baseline similarity:  0.4038491658866884\n",
      "errors:  300\n",
      "baseline similarity:  0.3784117789530327\n",
      "errors:  101\n",
      "baseline similarity:  0.32198573856721013\n",
      "errors:  162\n",
      "baseline similarity:  0.397615047467665\n",
      "errors:  184\n",
      "baseline similarity:  0.42098271827551886\n",
      "errors:  144\n",
      "baseline similarity:  0.41470949037890337\n",
      "errors:  238\n",
      "baseline similarity:  0.36827838271984076\n",
      "errors:  530\n",
      "baseline similarity:  0.42555711991504497\n",
      "errors:  147\n",
      "baseline similarity:  0.4320038749717943\n",
      "errors:  632\n",
      "baseline similarity:  0.3406026068939512\n",
      "errors:  165\n",
      "baseline similarity:  0.3769779549342117\n",
      "errors:  96\n",
      "baseline similarity:  0.34359095380374505\n",
      "errors:  150\n",
      "baseline similarity:  0.39974302755278274\n",
      "errors:  133\n",
      "baseline similarity:  0.3999648016058926\n",
      "errors:  303\n",
      "baseline similarity:  0.39474044069530334\n",
      "errors:  42\n",
      "baseline similarity:  0.2803825108164297\n",
      "errors:  210\n",
      "baseline similarity:  0.3863156749651268\n",
      "errors:  41\n",
      "baseline similarity:  0.3502289733024166\n",
      "errors:  49\n",
      "baseline similarity:  0.373671375862493\n",
      "errors:  624\n",
      "baseline similarity:  0.38282828794240037\n",
      "errors:  57\n",
      "baseline similarity:  0.38086940105984546\n",
      "errors:  81\n",
      "baseline similarity:  0.481915082080286\n",
      "errors:  335\n",
      "baseline similarity:  0.4170824653959701\n",
      "errors:  45\n",
      "baseline similarity:  0.352585250849784\n",
      "errors:  86\n",
      "baseline similarity:  0.4036394101764157\n",
      "errors:  651\n",
      "baseline similarity:  0.4214438029536618\n",
      "errors:  145\n",
      "baseline similarity:  0.4302141433140393\n",
      "errors:  59\n",
      "baseline similarity:  0.3630918134521811\n",
      "errors:  144\n",
      "baseline similarity:  0.42367181280837096\n",
      "errors:  151\n",
      "baseline similarity:  0.3339737903945309\n",
      "errors:  69\n",
      "baseline similarity:  0.3854818889637273\n",
      "errors:  151\n",
      "baseline similarity:  0.3712261772218755\n",
      "errors:  48\n",
      "baseline similarity:  0.42012295629289065\n",
      "errors:  216\n",
      "baseline similarity:  0.42390200883548845\n",
      "errors:  115\n",
      "baseline similarity:  0.43957033911988475\n",
      "errors:  29\n",
      "baseline similarity:  0.32028893069297565\n",
      "errors:  129\n",
      "baseline similarity:  0.38550956331150166\n",
      "errors:  155\n",
      "baseline similarity:  0.447002763609074\n",
      "errors:  193\n",
      "baseline similarity:  0.4048609518443606\n",
      "errors:  50\n",
      "baseline similarity:  0.3462244216268868\n",
      "errors:  133\n",
      "baseline similarity:  0.500997767756194\n",
      "errors:  70\n",
      "baseline similarity:  0.4746486078176112\n",
      "errors:  93\n",
      "baseline similarity:  0.34937888958532143\n",
      "errors:  71\n",
      "baseline similarity:  0.3893313069705475\n",
      "errors:  112\n",
      "baseline similarity:  0.390462255265683\n",
      "errors:  112\n",
      "baseline similarity:  0.3466479287157486\n",
      "errors:  68\n",
      "baseline similarity:  0.40347943869336367\n",
      "errors:  98\n",
      "baseline similarity:  0.395796962931988\n",
      "errors:  64\n",
      "baseline similarity:  0.3683315052269041\n",
      "errors:  376\n",
      "baseline similarity:  0.39394876786573524\n",
      "errors:  547\n",
      "baseline similarity:  0.37142370500629773\n",
      "errors:  134\n",
      "baseline similarity:  0.3324928496338888\n",
      "errors:  177\n",
      "baseline similarity:  0.39514163302761185\n",
      "errors:  121\n",
      "baseline similarity:  0.3096232039582559\n",
      "errors:  113\n",
      "baseline similarity:  0.3612237864243649\n",
      "errors:  117\n",
      "baseline similarity:  0.36339575041795713\n",
      "errors:  274\n",
      "baseline similarity:  0.3566166335684633\n",
      "errors:  254\n",
      "baseline similarity:  0.37169147103767447\n",
      "errors:  167\n",
      "baseline similarity:  0.3725470645731356\n",
      "errors:  204\n",
      "baseline similarity:  0.37533750990887765\n",
      "errors:  181\n",
      "baseline similarity:  0.3654442941252665\n",
      "errors:  525\n",
      "baseline similarity:  0.4064570192781187\n",
      "errors:  57\n",
      "baseline similarity:  0.35914625970433167\n",
      "errors:  90\n",
      "baseline similarity:  0.43628172077740707\n",
      "errors:  320\n",
      "baseline similarity:  0.4369001930863938\n",
      "errors:  302\n",
      "baseline similarity:  0.37831444407178577\n",
      "errors:  75\n",
      "baseline similarity:  0.3397686395996765\n",
      "errors:  66\n",
      "baseline similarity:  0.3374013547238309\n",
      "errors:  57\n",
      "baseline similarity:  0.37558651556196154\n",
      "errors:  133\n",
      "baseline similarity:  0.40287764754172384\n"
     ]
    }
   ],
   "source": [
    "sim_dist = specials.transcript.map(joke_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sim_dist to dataframe as a new feature\n",
    "specials['sim_dist'] = sim_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sim_dist to extract the S1 and S20 similarities, as head and tail respectively\n",
    "sim_head_list = []\n",
    "sim_tail_list = []\n",
    "sim_ht_list = []\n",
    "for i in range(126):\n",
    "    sim_head_list.append(sim_dist[i][0])\n",
    "    sim_tail_list.append(abs(sim_dist[i][-1]))\n",
    "    sim_ht_list.append(abs(sim_dist[i][0]/sim_dist[i][-1]))\n",
    "    \n",
    "sim_head = pd.Series(sim_head_list)\n",
    "sim_tail = pd.Series(sim_tail_list)\n",
    "sim_ht = pd.Series(sim_ht_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold in similarity measures into the dataframe\n",
    "specials['sim_head'] = sim_head\n",
    "specials['sim_tail'] = sim_tail\n",
    "specials['sim_ht'] = sim_ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head mean:  0.6118347466239409\n",
      "tail mean:  0.00822266212477755\n",
      "min head comic:  nikki glaser\n",
      "max head comic:  judah friedlander\n",
      "min tail comic:  mitch hedberg\n",
      "max tail comic:  fred armisen\n",
      "\n",
      "H/T mean:  219.87358023155969\n",
      "min H/T comic:  fred armisen\n",
      "max H/T comic:  mitch hedberg\n",
      "\n",
      "summary statistics for short-range similarity\n",
      "\n",
      "1st third:  0.5962200062712717\n",
      "2nd third:  0.6265849577262826\n",
      "\n",
      "max \n",
      "comic:  judah friedlander   sim head:  0.7195177800037722\n",
      "\n",
      "min \n",
      "comic:  nikki glaser   sim head:  0.49944679293952177\n",
      "\n",
      "summary statistics for long-range similarity\n",
      "\n",
      "1st third:  0.005214894354035243\n",
      "2nd third:  0.00946344501433552\n",
      "\n",
      "max\\ncomic:  fred armisen   sim head:  0.020080784200515678\n",
      "\n",
      "min\n",
      "comic:  mitch hedberg   sim head:  9.62989763982347e-05\n",
      "\n",
      "summary statistics for short/tail ratio similarity\n",
      "\n",
      "1st half:  77.42426274663654\n",
      "2nd third:  101.02641496987955\n",
      "\n",
      "max\\ncomic:  mitch hedberg   sim head:  7061.832110319238\n",
      "\n",
      "min\n",
      "comic:  fred armisen   sim head:  33.277901838666104\n",
      "\n",
      " 63  short comics: \n",
      " ['adam devine', 'al madrigal', 'ali wong', 'amy schumer', 'anthony jeselnik', 'ari shaffir', 'bert kreischer', 'bill burr', 'bill maher', 'cedric the entertainer', 'chelsea peretti', 'colin quinn', 'cristela alonzo', 'daniel tosh', 'dave attell', 'dave chappelle', 'david cross', 'demetri martin', 'deray davis', 'donald glover', 'dylan moran', 'enissa amani', 'fahim anwar', 'frankie boyle', 'greg davies', 'hannah gadsby', 'hannibal buress', 'hasan minhaj', 'henry rollins', 'jack whitehall', 'jerry seinfeld', 'jim norton', 'jimmy carr', 'john leguizamo', 'john mulaney', 'ken jeong', 'lewis black', 'lisa lampanelli', 'marc maron', 'maria bamford', 'marlon wayans', 'michelle wolf', 'mike epps', 'mitch hedberg', 'nate bargatze', 'nick offerman', 'nikki glaser', 'norm macdonald', 'patton oswalt', 'pete holmes', 'ray romano', 'robin williams', 'ron white', 'russell brand', 'russell howard', 'sarah silverman', 'todd barry', 'todd glass', 'tom segura', 'vir das', 'volker pispers', 'wanda sykes', 'whitney cummings']\n",
      "\n",
      " 0  mid_comics: \n",
      " []\n",
      "\n",
      " 63  long comics: \n",
      " ['adam sandler', 'adel karam', 'anjelah johnson', 'aziz ansari', 'big jay oakerson', 'bill hicks', 'bo burnham', 'brad williams', 'brent morin', 'brian regan', 'bridget everett', 'chris rock', 'chris tucker', 'craig ferguson', 'd l hughley', 'dana carvey', 'daniel sloss', 'doug stanhope', 'eddie griffin', 'eddie izzard', 'eddie murphy', 'ellen degeneres', 'emily heller', 'erik griffin', 'fred armisen', 'gabriel iglesias', 'george carlin', 'gina yashere', 'iliza shlesinger', 'jeff foxworthy', 'jen kirkman', 'jim gaffigan', 'jim jefferies', 'jo koy', 'joe mandes', 'joe rogan', 'judah friedlander', 'katt williams', 'kavin jay', 'kevin hart', 'kevin james', 'kevin smith', 'lenny bruce', 'louis ck', 'maz jobrani', 'michael che', 'michael mcintyre', 'mike birbiglia', 'mo amer', 'neal brennan', 'pablo francisco', 'patrice oneal', 'paul mooney', 'ralphie may', 'richard pryor', 'ricky gervais', 'rowan atkinson', 'roy wood', 'russell peters', 'sarah millican', 'sebastian maniscalco', 'stewart lee', 'trevor noah']\n"
     ]
    }
   ],
   "source": [
    "# exploratory analysis of new features!\n",
    "i_head_min = sim_head.idxmin()\n",
    "i_head_max = sim_head.idxmax()\n",
    "i_tail_min = sim_tail.idxmin()\n",
    "i_tail_max = sim_tail.idxmax()\n",
    "i_ht_min = sim_ht.idxmin()\n",
    "i_ht_max = sim_ht.idxmax()\n",
    "\n",
    "head_mean = sim_head.mean()\n",
    "tail_mean = sim_tail.mean()\n",
    "ht_mean = sim_ht.mean()\n",
    "\n",
    "print('head mean: ', head_mean)\n",
    "print('tail mean: ', tail_mean)\n",
    "print('min head comic: ', specials.iloc[i_head_min,0])\n",
    "print('max head comic: ', specials.iloc[i_head_max,0])\n",
    "print('min tail comic: ', specials.iloc[i_tail_min,0])\n",
    "print('max tail comic: ', specials.iloc[i_tail_max,0])\n",
    "\n",
    "print('\\nH/T mean: ', ht_mean)\n",
    "print('min H/T comic: ', specials.iloc[i_ht_min,0])\n",
    "print('max H/T comic: ', specials.iloc[i_ht_max,0])\n",
    "\n",
    "print('\\nsummary statistics for short-range similarity\\n')\n",
    "print('1st third: ', np.quantile(sim_head, 0.33))\n",
    "print('2nd third: ', np.quantile(sim_head, 0.67))\n",
    "\n",
    "print('\\nmax \\ncomic: ', specials.iloc[sim_head.idxmax(),0], '  sim head: ', sim_head.max())\n",
    "print('\\nmin \\ncomic: ', specials.iloc[sim_head.idxmin(),0], '  sim head: ', sim_head.min())\n",
    "\n",
    "print('\\nsummary statistics for long-range similarity\\n')\n",
    "print('1st third: ', np.quantile(sim_tail, 0.25))\n",
    "print('2nd third: ', np.quantile(sim_tail, 0.62))\n",
    "\n",
    "print('\\nmax\\\\ncomic: ', specials.iloc[sim_tail.idxmax(),0], '  sim head: ', sim_tail.max())\n",
    "print('\\nmin\\ncomic: ', specials.iloc[sim_tail.idxmin(),0], '  sim head: ', sim_tail.min())\n",
    "\n",
    "print('\\nsummary statistics for short/tail ratio similarity\\n')\n",
    "print('1st half: ', np.quantile(sim_ht, 0.5))\n",
    "print('2nd third: ', np.quantile(sim_ht, 0.67))\n",
    "\n",
    "print('\\nmax\\\\ncomic: ', specials.iloc[sim_ht.idxmax(),0], '  sim head: ', sim_ht.max())\n",
    "print('\\nmin\\ncomic: ', specials.iloc[sim_ht.idxmin(),0], '  sim head: ', sim_ht.min())\n",
    "\n",
    "short_index = []\n",
    "mid_index = []\n",
    "long_index = []\n",
    "\n",
    "form_factor = sim_ht\n",
    "for i in range(len(sim_tail)):\n",
    "    if form_factor[i] >= 77 :\n",
    "        short_index.append(i)\n",
    "\n",
    "    if form_factor[i] < 77:\n",
    "        long_index.append(i)\n",
    "\n",
    "short_comics = specials.iloc[short_index,0]\n",
    "mid_comics = specials.iloc[mid_index,0]\n",
    "long_comics = specials.iloc[long_index,0]\n",
    "print('\\n', len(short_comics), ' short comics: \\n', list(short_comics))\n",
    "print('\\n', len(long_comics), ' long comics: \\n', list(long_comics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'specials' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4571155fd437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshort_long_div\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mspecials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'joke_form'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_ht\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mshort_long_div\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'specials' is not defined"
     ]
    }
   ],
   "source": [
    "# include new column to distinguish between short and long form comics\n",
    "# spreading them into 2 evenly distributed groups, very fuzzy line that separates short from long form comics\n",
    "# ideally, the metric would get fine-tuned for further differentiation into short form, mid form, and long form\n",
    "short_long_div = 100\n",
    "\n",
    "specials['joke_form'] = specials.apply(lambda row: 0 if (row.sim_ht > short_long_div) else 1, axis=1)\n",
    "print(specials.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dataframe with joke_form feature, will import into separate jupyter notebook for later trimming for deployment\n",
    "\n",
    "data_path = '/Users/johnpapaioannou/Desktop/insight/project/data/'\n",
    "file_path = data_path + 'joke_form.pkl'\n",
    "specials.to_pickle(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of specials:  126\n",
      "total:  11627863 mean:  92284.62698412698\n",
      "pre clean length:  94514\n",
      "after init clean:  92429\n",
      "after stop word removal:  62361\n",
      "after removing numbers:  62262\n",
      "after stripping short sentences:  59809\n",
      "# of sentences:  1785\n",
      "errors:  135\n",
      "shape before:  (1785, 300)\n",
      "125 1.0\n",
      "125 0.4637859738313628\n",
      "125 0.4191660060992311\n",
      "125 0.41029533751512604\n",
      "125 0.4035567961775156\n",
      "125 0.40087836517417913\n",
      "125 0.39713082647933706\n",
      "125 0.39178287376276116\n",
      "125 0.3888707411193809\n",
      "125 0.39231606604763625\n",
      "125 0.39030795471254875\n",
      "125 0.38910739667302036\n",
      "125 0.3859207685206093\n",
      "125 0.38402182368345833\n",
      "125 0.3872987933615818\n",
      "125 0.3863475426107288\n",
      "125 0.38378192709016845\n",
      "125 0.3830446343444706\n",
      "125 0.3822209315308542\n",
      "125 0.38136030969795714\n",
      "ensemble size:  10\n",
      "0.3778468022570286\n",
      "[0.6221532  0.08593917 0.0413192  0.03244854 0.02570999 0.02303156\n",
      " 0.01928402 0.01393607 0.01102394 0.01446926 0.01246115 0.01126059\n",
      " 0.00807397 0.00617502 0.00945199 0.00850074 0.00593512 0.00519783\n",
      " 0.00437413 0.00351351]\n"
     ]
    }
   ],
   "source": [
    "no_of_specials = len(specials.iloc[:,1])\n",
    "print('# of specials: ', no_of_specials)\n",
    "\n",
    "# for i in range(no_of_specials):\n",
    "#     print('i: ', i, 'comedian: ', specials.iloc[i,0])\n",
    "    \n",
    "jeselnik = specials.iloc[7,1]\n",
    "birbigs = specials.iloc[89,1]\n",
    "hedberg = specials.iloc[91,1]\n",
    "patrice = specials.iloc[99,1]\n",
    "bargatze = specials.iloc[93,1]\n",
    "maron = specials.iloc[82,1]\n",
    "carr = specials.iloc[65,1]\n",
    "demetri_martin = specials.iloc[34,1]\n",
    "amy_schumer = specials.iloc[5,1]\n",
    "colin_quinn = specials.iloc[24,1]\n",
    "kevin_smith = specials.iloc[77,1]\n",
    "stewart_lee = specials.iloc[117,1]\n",
    "attell = specials.iloc[31,1]\n",
    "\n",
    "\n",
    "char_tot = 0\n",
    "for i in range(no_of_specials):\n",
    "    char_tot += len(specials.iloc[i,1])\n",
    "\n",
    "print('total: ', char_tot, 'mean: ', char_tot/no_of_specials)\n",
    "\n",
    "special_sentences = gensim_preprocess(specials.iloc[7,1])\n",
    "\n",
    "print('# of sentences: ', len(special_sentences))\n",
    "\n",
    "#print(special_sentences)\n",
    "sentence_vecs = word2sent_vec(w2v_model, special_sentences)\n",
    "chunk_size = 20\n",
    "\n",
    "sim_dist = sentence_sim_dist(sentence_vecs, chunk_size)\n",
    "for j in range(chunk_size):\n",
    "    print(i, sim_dist[j])\n",
    "\n",
    "sim_dist_shuffle = np.zeros(chunk_size)\n",
    "ensemble_size = 10\n",
    "for i in range(ensemble_size):\n",
    "    sentence_shuffle = shuffler(sentence_vecs)\n",
    "    sim_dist_shuffle += sentence_sim_dist(sentence_shuffle, chunk_size)\n",
    "    \n",
    "print('ensemble size: ', ensemble_size)\n",
    "\n",
    "baseline_ensembled = np.mean(sim_dist_shuffle[1:])/ensemble_size\n",
    "print(baseline_ensembled)\n",
    "\n",
    "sim_dist_adjusted = sim_dist - baseline_ensembled\n",
    "\n",
    "print(sim_dist_adjusted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
